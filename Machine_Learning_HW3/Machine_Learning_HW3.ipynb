{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# File Upload"
      ],
      "metadata": {
        "id": "XDzF_cqQVb4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybnus4A4VXYr"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "# csv file\n",
        "uploaded = files.upload()\n",
        "for f in uploaded.keys():\n",
        "    print('User iploaded file \"{name}\" with length {length} bytes\\n'.format(name = f, length = len(uploaded[f])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task \n",
        "\n",
        "## Model: $y = f(x)$\n",
        "## Approach: Neural Network"
      ],
      "metadata": {
        "id": "YjgmNen5V9Nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Read the .csv file and store the data into the dataframe "
      ],
      "metadata": {
        "id": "Xn4Q_dMTVrua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Real Data\")\n",
        "\n",
        "data = pd.read_csv(\"./hw3_data.csv\").sort_values(by='x', axis=0)\n",
        "plt.plot(data['x'], data['y'], 'bo', label=\"Data Set\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HTNiGhCeVsDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the perceptron used in the multi-layer perceptron (MLP)"
      ],
      "metadata": {
        "id": "kxAC8JszdrFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Define Activation Functions"
      ],
      "metadata": {
        "id": "dkVzGj3jvIdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def derivate(self, x):\n",
        "        return (1 - self.forward(x)) * self.forward(x)\n",
        "\n",
        "\n",
        "class Relu:\n",
        "    def forward(self, x):\n",
        "        return np.where(x > 0, x, 0.0)\n",
        "\n",
        "    def derivate(self, x):\n",
        "        return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def derivate(self, x):\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "\n",
        "\n",
        "class Identity:\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "    def derivate(self, x):\n",
        "        return 1"
      ],
      "metadata": {
        "id": "zl4BjfN3kG-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Define Initializer"
      ],
      "metadata": {
        "id": "Pl1lgIp6wbQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Rand:\n",
        "    def initialize(self, layer):\n",
        "        w = np.random.rand(layer.input_dim, layer.output_dim)\n",
        "        b = np.random.randd(1, layer.output_dim)\n",
        "        dw = np.zeros([layer.input_dim, layer.output_dim])\n",
        "        db = np.zeros([1, layer.output_dim])\n",
        "        return w, b, dw, db"
      ],
      "metadata": {
        "id": "xeEvMxe9wd1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Define Layer"
      ],
      "metadata": {
        "id": "xlo3cNthxAAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, units, activation, input_dim, initializer=Rand):\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "        self.dw = None\n",
        "        self.db = None\n",
        "        self.delta = None\n",
        "\n",
        "        self.output_dim = units\n",
        "        self.input_dim = input_dim\n",
        "        self.activation = activation\n",
        "        self.initializer = initializer\n",
        "\n",
        "\n",
        "    def reset_layer(self):\n",
        "        w, b, dw, db = self.initializer.initialize(self)\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "        self.dw = dw\n",
        "        self.db = db\n",
        "\n",
        "\n",
        "    def forward(self, x, update):\n",
        "        z = np.matmul(x, self.w) + self.b\n",
        "        a = self.activation.forward(z)\n",
        "        if update:\n",
        "            self.z = z\n",
        "            self.a = a\n",
        "        return a\n",
        "\n",
        "    \n",
        "    def update_delta(self, next_layer):\n",
        "        delta = np.matmul(next_layer.delta, next_layer.w.T) * self.activation.derivate(self.z)\n",
        "        self.delta = delta\n",
        "\n",
        "\n",
        "    def update_gradient(self, a_in):\n",
        "        delta_out = self.delta\n",
        "        self.db = delta_out.sum(axis=0).reshape([1, -1])\n",
        "        self.dw = np.matmul(a_in.T, delta_out)"
      ],
      "metadata": {
        "id": "IU2LIFNvxCEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Define Loss"
      ],
      "metadata": {
        "id": "2aXSbPgDzIjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class MSE:\n",
        "    def forward(self, actual, prediction):\n",
        "        return 0.5 * ((prediction - actual) ** 2)\n",
        "\n",
        "    def derivate(self, actual, prediction):\n",
        "        return prediction - actual"
      ],
      "metadata": {
        "id": "oraldypJzK2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8 Define Optimizer"
      ],
      "metadata": {
        "id": "L2bcNAN-GbY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "    def __init__(self, learning_rate=0.001):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    \n",
        "    def initialize_weights(self, layers):\n",
        "        return layers\n",
        "\n",
        "    \n",
        "    def update_weights(self,layers):\n",
        "        for i in range(len(layers)):\n",
        "            layers[i].w = layers[i].w - self.learning_rate * layers[i].dw\n",
        "            layers[i].b = layers[i].b - self.learning_rate * layers[i].db\n",
        "        return layers"
      ],
      "metadata": {
        "id": "pCV5VsizGb2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Define Model"
      ],
      "metadata": {
        "id": "gbdJeNdbziH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.n_layers = 0\n",
        "        self.trainer = None\n",
        "        self.train_log = None\n",
        "\n",
        "\n",
        "    def add(self, layer):\n",
        "        layer.input_dim = self.layers[-1].output_dim\n",
        "        layer.reset_layer()\n",
        "        self.layers.append(layer)\n",
        "        self.n_layers += 1\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        p = self.forward_prop(x, update=False)\n",
        "        return p\n",
        "\n",
        "\n",
        "    def train(self, loss, x, optimizer=GradientDescent):\n",
        "        self.trainer = ModelTrain()\n",
        "        self.trainer.train(self, loss, x, optimizer)\n",
        "\n",
        "\n",
        "    def forward_prop(self, x, update=True):\n",
        "        a = x\n",
        "        for layer in self.layers:\n",
        "            a = layer.forward(a, update=update)\n",
        "        return a\n",
        "\n",
        "\n",
        "    def back_prop(self, x, y, loss):\n",
        "        self.update_deltas(loss, y)\n",
        "        self.update_gradients(x)\n",
        "\n",
        "\n",
        "    def update_deltas(self, loss, y):\n",
        "        for i, layer in enumerate(reversed(self.layers)):\n",
        "            if i == 0:\n",
        "                delta = loss.derivate(y, layer.a) * layer.activation.derivate(layer.z)\n",
        "                layer.delta - delta\n",
        "            else:\n",
        "                layer_next = self.layers[-i]\n",
        "                layer.update_delta(layer_next)\n",
        "\n",
        "    \n",
        "    def update_gradients(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i == 0:\n",
        "                a_in = x\n",
        "            else:\n",
        "                prev_layer = self.layers[i - 1]\n",
        "                a_in = prev_layer.a\n",
        "            layer.update_gradient(a_in)"
      ],
      "metadata": {
        "id": "u5KRyLHozhv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7 Define Batcher"
      ],
      "metadata": {
        "id": "FSCk2sMp3pSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Batcher:\n",
        "    def __init__(self, x, batch_size):\n",
        "        self.x = x\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle_on_reset = False\n",
        "\n",
        "        if type(x) == list:\n",
        "            self.data_size = data[0].shape[0]\n",
        "        else:\n",
        "            self.data_size = data.shape[0]\n",
        "        \n",
        "        self.n_batches = int(np.cceil(self.data_size / self.batch_size))\n",
        "        self.idx = np.arrange(0, self.data_size, dtype=int)\n",
        "        self.current = 0\n",
        "\n",
        "    \n",
        "    def shuffle(self):\n",
        "        np.random.shuffle(self.idx)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        if self.shuffle_on_reset:\n",
        "            self.shuffle()\n",
        "        self.current = 0\n",
        "\n",
        "\n",
        "    def next(self):\n",
        "        batch = []\n",
        "        i_select = self.idx[(self.current * self.batch_size) : ((self.current + 1) * self.batch_size)]\n",
        "\n",
        "        for data in self.x:\n",
        "            batch.append(data[i_select])\n",
        "        \n",
        "        if self.current < (self.n_batches - 1):\n",
        "            self.current = self.current + 1\n",
        "        else:\n",
        "            self.reset()\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "kdVMLVSS3qMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Define Trainer"
      ],
      "metadata": {
        "id": "2CQ6YPtH2fHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "default_params = {\n",
        "    'n_epoch': 10,\n",
        "    'print_rate': 5,\n",
        "    'batch_size': 128,\n",
        "    'learning_rate': 0.001\n",
        "}\n",
        "\n",
        "\n",
        "class ModelTrain:\n",
        "    def __init__(self):\n",
        "        self.batcher = None\n",
        "        self.optimizer = None\n",
        "        self.params = default_params\n",
        "        \n",
        "\n",
        "    def train(self, model, loss, x, optimizer=GradientDescent):\n",
        "        self.optimizer = optimizer\n",
        "        model.layers = self.optimizer.initialize_parapmeters(model.layers)\n",
        "\n",
        "        if self.batcher is None:\n",
        "            self.batcher = Batcher(x, self.params['batch_size'])\n",
        "\n",
        "        epoch = 1\n",
        "        train_loss = []\n",
        "        model.train_log = []\n",
        "        while epoch <= self.params['n_epoch']:\n",
        "            self.batcher.reset()\n",
        "\n",
        "            for batch_i in range(self.batcher.n_batches):\n",
        "                batch = self.batcher.next()\n",
        "                x_batch = batch[0]\n",
        "                y_batch = batch[1]\n",
        "\n",
        "                self.train_step(model, x_batch, y_batch, loss)\n",
        "                loss_i = self.compute_loss(model.layerrrs[-1].a, y_batch, loss)\n",
        "                model.train_log.append(np.array([epoch, batch_i, loss_i]))\n",
        "\n",
        "            epoch += 1\n",
        "\n",
        "        model.train_log = np.vstack(model.train_log)\n",
        "        model.train_log = pd.DataFrame(model.train_log, colums=['epoch', 'iter', 'loss'])\n",
        "\n",
        "\n",
        "    def train_step(self, model, x, y, loss):\n",
        "        _ = model.forward_prop(x)\n",
        "        model.back_prop(x, y, loss)\n",
        "        model.layers = self.optimizer.update_weights(model.layers)\n",
        "\n",
        "\n",
        "    def compute_loss(actual, prediction, loss):\n",
        "        current_loss = loss.forward(actual, prediction)\n",
        "        return current_loss.mean()"
      ],
      "metadata": {
        "id": "Gh9wFeby2fbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Main"
      ],
      "metadata": {
        "id": "76QN2Yk_Gpju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP\n",
        "model.add(Dense(units=8, activation=Relu, input_dim=1))\n",
        "model.add(Dense(units=4, activation=Relu))\n",
        "model.add(Dense(units=4, activation=Relu))\n",
        "model.add(Dense(units=1, activation=Identity))\n",
        "\n",
        "loss = MSE\n",
        "print(model.layers)\n",
        "# model.train(loss, [data['x'], data['y']])\n",
        "\n",
        "# p = model.predict(data['x'])\n",
        "# performance = loss.forward(p, data['y'])\n",
        "# print(f\"Prediction loss: {performance.mean()}\")"
      ],
      "metadata": {
        "id": "nMsy4z9SGpKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}